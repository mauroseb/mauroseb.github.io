---
layout: posts
title: "OpenShift 4 Baremetal Installation on KVM"
date: 2020-09-01
categories: [blog]
tags: [ ocp, kubernetes, kvm ]
author: mauroseb
excerpt_separator: "<!-- more -->"
---
### Intro

In this opportunity I will describe the step by step installation process of an OpenShift 4 on libvirt/KVM using the baremetal deployment method, which I use many times as lab.
The main peculiarities here will be the Static IP assignment for the nodes which is often a requirement in production environments as well as a global proxy configuration which also stands out for the same.

The approach is the same as one would take for Baremetal deployments, and also has a lot in common with other UPI deployments.


### Prepare the KVM Virtual Host

All virtual!

Since I am going to run every OCP node in a virtualized environment the hypervisor has to be properly configured.
This includes preparing the virtual host node, installing the virtualization software, the right firewall configuration, creating the virtual network itself that will be used for this cluster.
I have some automation written for that purpose that should become available soon. In the meantime, the step by step guide can be found in the openshift repos directly:

 - [Libvirt HOWTO](https://github.com/openshift/installer/blob/master/docs/dev/libvirt/README.md)

The following diagram shows how the environment looks like.

<img src="/images/LAB_OCP.png" alt="OCP LAB PNG" style="width:75%;"/>


### Prepare Helper Node

Since with baremetal installation (as well as UPI) we will relay on external network services I have to set this up manually.

The services that we need are:

 - **DNS - Bind.** I have already a master setup in my lab.
 - **External Load Balancer - HAproxy.**
 - **Proxy - Squid.** Since I want to test the global proxy configuration.
 - **HTTP - A python process.**  To serve CoreOS bits and ignition files so the nodes can be built.
 - **NTP - Chronyd** I am using an already existing one.
 - **DHCP/PXE** - Since I am deploying my nodes with static IPs, there is no need to have a **DHCP** server running, however this would be normally required.

Then, I have created a VM in the OCP network which will host these services.

If you do not want to set all that up by yourself, there is an upstream project created in Ansible called __ocp-helpernode__ which sort of automates the process of creating the aforementioned services in a CentOS/RHEL 7/8 instance.

 - <https://github.com/RedHatOfficial/ocp4-helpernode>

In my case I will be using a RHEL8.2 server with EPEL channels enabled. It will use IP 192.168.133.9.

{% highlight bash %}
[root@ice-ocp4-helpernode ~]# cat /etc/redhat-release
Red Hat Enterprise Linux release 8.2 (Ootpa)
[root@ice-ocp4-helpernode ~]# yum repolist
Updating Subscription Management repositories.
repo id                                                                                         repo name
ansible-2.9-for-rhel-8-x86_64-rpms                                                              Red Hat Ansible Engine 2.9 for RHEL 8 x86_64 (RPMs)
epel                                                                                            Extra Packages for Enterprise Linux 8 - x86_64
epel-modular                                                                                    Extra Packages for Enterprise Linux Modular 8 - x86_64
rhel-8-for-x86_64-appstream-rpms                                                                Red Hat Enterprise Linux 8 for x86_64 - AppStream (RPMs)
rhel-8-for-x86_64-baseos-rpms                                                                   Red Hat Enterprise Linux 8 for x86_64 - BaseOS (RPMs)
{% endhighlight %}

Now you only need to follow the instructions in the git repo.
In short, install ansible and git and clone the repo in question. Copy and modify the default __vars.yaml__ file and then run the playbook passing this file.

{% highlight bash %}
[root@ice-ocp4-helpernode ocp4-helpernode]# cat vars.yaml
---
disk: vda
helper:
  name: "ice-ocp4-helpernode"
  ipaddr: "192.168.133.9"
dns:
  domain: "t1.lab.local"
  clusterid: "ocp4-bm"
  forwarder1: "192.168.133.1"
dhcp:
  router: "192.168.133.1"
  bcast: "192.168.133.255"
  netmask: "255.255.255.0"
  poolstart: "192.168.133.50"
  poolend: "192.168.133.80"
  ipid: "192.168.133.0"
  netmaskid: "255.255.255.0"
bootstrap:
  name: "ice-ocp4-bootstrap"
  ipaddr: "192.168.133.50"
  macaddr: "52:54:00:60:72:67"
masters:
  - name: "ice-ocp4-master-0"
    ipaddr: "192.168.133.61"
    macaddr: "52:54:00:e7:9d:67"
  - name: "ice-ocp4-master-1"
    ipaddr: "192.168.133.62"
    macaddr: "52:54:00:80:16:23"
  - name: "ice-ocp4-master-2"
    ipaddr: "192.168.133.63"
    macaddr: "52:54:00:d5:1c:39"
workers:
  - name: "ice-ocp4-worker-0"
    ipaddr: "192.168.133.71"
    macaddr: "52:54:00:f4:26:a1"
  - name: "ice-ocp4-worker-1"
    ipaddr: "192.168.133.72"
    macaddr: "52:54:00:82:90:00"
  - name: "ice-ocp4-worker-2"
    ipaddr: "192.168.133.73"
    macaddr: "52:54:00:8e:10:34"
{% endhighlight %}

**NOTE: Only Class C networks can be used at this moment with this playbooks**

Mind that if you are using DHCP (dynamic IP assignment) then the MAC addresses have to match what your nodes have.
After the playbook runs you will get the services aforementioned services set up as expected

Follows the bind's zonefile generated:
{% highlight bash %}
$TTL 1W
@	IN	SOA	ns1.ocp4-bm.t1.lab.local.	root (
			2020071200	; serial
			3H		; refresh (3 hours)
			30M		; retry (30 minutes)
			2W		; expiry (2 weeks)
			1W )		; minimum (1 week)
	IN	NS	ns1.ocp4-bm.t1.lab.local.
	IN	MX 10	smtp.ocp4-bm.t1.lab.local.
;
; 
ns1	IN	A	192.168.133.9
smtp	IN	A	192.168.133.9
;
ice-ocp4-helpernode	IN	A	192.168.133.9
;
; The api points to the IP of your load balancer
api		IN	A	192.168.133.9
api-int		IN	A	192.168.133.9
;
; The wildcard also points to the load balancer
*.apps		IN	A	192.168.133.9
;
; Create entry for the local registry
registry.ocp4-bm.t1.lab.local	IN	A	192.168.133.9
;
; Create entry for the bootstrap host
ice-ocp4-bootstrap	IN	A	192.168.133.50
;
; Create entries for the master hosts
ice-ocp4-master-0		IN	A	192.168.133.61
ice-ocp4-master-1		IN	A	192.168.133.62
ice-ocp4-master-2		IN	A	192.168.133.63
;
; Create entries for the worker hosts
ice-ocp4-worker-0		IN	A	192.168.133.71
ice-ocp4-worker-1		IN	A	192.168.133.72
ice-ocp4-worker-2		IN	A	192.168.133.73
;
; The ETCd cluster lives on the masters...so point these to the IP of the masters
etcd-0	IN	A	192.168.133.61
etcd-1	IN	A	192.168.133.62
etcd-2	IN	A	192.168.133.63
;
; Create entries for the other hosts
non-cluster-vm		IN	A	192.168.133.51
;
; The SRV records are IMPORTANT....make sure you get these right...note the trailing dot at the end...
_etcd-server-ssl._tcp	IN	SRV	0 10 2380 etcd-0.ocp4-bm.t1.lab.local.
_etcd-server-ssl._tcp	IN	SRV	0 10 2380 etcd-1.ocp4-bm.t1.lab.local.
_etcd-server-ssl._tcp	IN	SRV	0 10 2380 etcd-2.ocp4-bm.t1.lab.local.
;
;EOF
{% endhighlight %}


Here is an example of the resulting __/etc/haproxy/haproxy.conf__.

{% highlight bash %}
[root@ice-ocp4-helpernode ~]# cat /etc/haproxy/haproxy.cfg
#---------------------------------------------------------------------
# Example configuration for a possible web application.  See the
# full configuration options online.
#
#   http://haproxy.1wt.eu/download/1.4/doc/configuration.txt
#
#---------------------------------------------------------------------

#---------------------------------------------------------------------
# Global settings
#---------------------------------------------------------------------
global
    # to have these messages end up in /var/log/haproxy.log you will
    # need to:
    #
    # 1) configure syslog to accept network log events.  This is done
    #    by adding the '-r' option to the SYSLOGD_OPTIONS in
    #    /etc/sysconfig/syslog
    #
    # 2) configure local2 events to go to the /var/log/haproxy.log
    #   file. A line like the following can be added to
    #   /etc/sysconfig/syslog
    #
    #    local2.*                       /var/log/haproxy.log
    #
    log         127.0.0.1 local2

    chroot      /var/lib/haproxy
    pidfile     /var/run/haproxy.pid
    maxconn     4000
    user        haproxy
    group       haproxy
    daemon

    # turn on stats unix socket
    stats socket /var/lib/haproxy/stats

#---------------------------------------------------------------------
# common defaults that all the 'listen' and 'backend' sections will
# use if not designated in their block
#---------------------------------------------------------------------
defaults
    mode                    http
    log                     global
    option                  httplog
    option                  dontlognull
    option http-server-close
    option forwardfor       except 127.0.0.0/8
    option                  redispatch
    retries                 3
    timeout http-request    10s
    timeout queue           1m
    timeout connect         10s
    timeout client          1m
    timeout server          1m
    timeout http-keep-alive 10s
    timeout check           10s
    maxconn                 3000

#---------------------------------------------------------------------

listen stats
    bind :9000
    mode http
    stats enable
    stats uri /
    monitor-uri /healthz


frontend openshift-api-server
    bind *:6443
    default_backend openshift-api-server
    mode tcp
    option tcplog

backend openshift-api-server
    balance source
    mode tcp
    server ice-ocp4-bootstrap 192.168.133.50:6443 check
    server ice-ocp4-master-0 192.168.133.61:6443 check
    server ice-ocp4-master-1 192.168.133.62:6443 check
    server ice-ocp4-master-2 192.168.133.63:6443 check

frontend machine-config-server
    bind *:22623
    default_backend machine-config-server
    mode tcp
    option tcplog

backend machine-config-server
    balance source
    mode tcp
    server ice-ocp4-bootstrap 192.168.133.50:22623 check
    server ice-ocp4-master-0 192.168.133.61:22623 check
    server ice-ocp4-master-1 192.168.133.62:22623 check
    server ice-ocp4-master-2 192.168.133.63:22623 check
  
frontend ingress-http
    bind *:80
    default_backend ingress-http
    mode tcp
    option tcplog

backend ingress-http
    balance source
    mode tcp
    server ice-ocp4-worker-0-http-router0 192.168.133.71:80 check
    server ice-ocp4-worker-1-http-router1 192.168.133.72:80 check
    server ice-ocp4-worker-2-http-router2 192.168.133.73:80 check
   
frontend ingress-https
    bind *:443
    default_backend ingress-https
    mode tcp
    option tcplog

backend ingress-https
    balance source
    mode tcp
    server ice-ocp4-worker-0-https-router0 192.168.133.71:443 check
    server ice-ocp4-worker-1-https-router1 192.168.133.72:443 check
    server ice-ocp4-worker-2-https-router2 192.168.133.73:443 check

#---------------------------------------------------------------------
{% endhighlight %}

Finally installing and setting up Squid to act as proxy.
{% highlight bash %}
$ sudo yum install -y squid
{% endhighlight %}

The **/etc/squid/squid.conf** looks as follows:
{% highlight bash %}
acl localnet src 192.168.133.0/24       # OCP4 net
acl SSL_ports port 443
acl Safe_ports port 80		# http
acl Safe_ports port 21		# ftp
acl Safe_ports port 443		# https
acl CONNECT method CONNECT
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
http_access allow localnet
http_access allow localhost
http_access allow all
http_port 3128
coredump_dir /var/spool/squid
refresh_pattern ^ftp:		1440	20%	10080
refresh_pattern ^gopher:	1440	0%	1440
refresh_pattern -i (/cgi-bin/|\?) 0	0%	0
refresh_pattern .		0	20%	4320
{% endhighlight %}

{% highlight bash %}
$ systemctl restart squid
$ sudo firewall-cmd --add-port=3128/tcp --permanent
$ sudo firewall-cmd --reload
{% endhighlight %}



### OCP Prereqs

As usual I will be assuming you have the latest and gratest:

 - openshift client
 - openshift-installer
 - pull secret


### OCP Installation

I normally create a directory to place all the required bits. In this case I called it _ocp4-baremetal_. Inside will create the installer directory.

#### 1. Create __install-config.yaml__ and save it in the _ocp4-baremetal_ directory so we do not loose it in every deploy.
   Note the number of workers is 0 as they will be added later on, plaform is None, proxy details.

{% highlight bash %}
apiVersion: v1
baseDomain: t1.lab.local
proxy:
  httpProxy: http://192.168.133.9:3128
  httpsProxy: http://192.168.133.9:3128
  noproxy: t1.lab.local,192.168.133.9
compute:
- hyperthreading: Enabled
  name: worker
  replicas: 0
controlPlane:
  hyperthreading: Enabled
  name: master
  replicas: 3
metadata:
  name: ocp4-bm
networking:
  clusterNetworks:
  - cidr: 10.128.0.0/14
    hostPrefix: 23
  networkType: OpenShiftSDN
  machineNetwork:
  - cidr: 192.168.133.0/24
  serviceNetwork:
  - 172.30.0.0/16
platform:
  none: {}
pullSecret: {{ PULL_SECRET }}
sshKey: |
  {{ SSH_PUBKEY }}
  
{% endhighlight %}

{:start="2"}
#### 2. Create a directory tree for RHCOS files

One would normally boot from ISO but since virt-install allows to specify directly the bios, kernel and ramdisk living in the webserver, and also passing kernel command line options, I can take advantage of that. Also we need to create a __.treeinfo__ file mapping this content.

{% highlight bash %}
$ cat rhcos/4.5/.treeinfo
[general]
arch = x86_64
family = Red Hat CoreOS
platforms = x86_64
version = 4.5.0
[images-x86_64]
initrd = initramfs.img
kernel = vmlinuz
```
```
$ tree rhcos
rhcos
├── 4.4
│   ├── bios.raw.gz
│   ├── initramfs.img
│   ├── rhcos-4.4.3-x86_64-qemu.x86_64.qcow2.gz
│   └── vmlinuz
└── 4.5
    ├── bios.raw.gz
    ├── initramfs.img
    └── vmlinuz

2 directories, 7 files
{% endhighlight %}


 - Make the content available via HTTP

{% highlight bash %}
$ python3 -m http.server 8080 --directory $WORKDIR
Serving HTTP on 0.0.0.0 port 8080 (http://0.0.0.0:8080/) ...
{% endhighlight %}

{:start="3"}
#### 3. Create and populate the installer directory

The following script will clean the installer directory _ocp4-bm_ if it exists, place the _install-config.yaml_ file in it (created in a previous step), create the manifests for the installer, modify them to set the masters to __no schedulable__ (there is a bug associated with this step in the documentation [^1]), and finally create the ignition config files. Now everything is ready to launch the VMs.

{% highlight bash %}
#!/bin/bash

WORKDIR=~/ocp4-baremetal
RUNFILE=/run/user/${UID}/ocp_content.pid

rm -rf ocp4-bm/
mkdir ocp4-bm
cp ${WORKDIR}/install-config-45.yaml ${WORKDIR}/ocp4-bm/install-config.yaml
${WORKDIR}/bin/openshift-install-45 create manifests  --dir=${WORKDIR}/ocp4-bm

sed -i 's/mastersSchedulable: true/mastersSchedulable: False/' ${WORKDIR}/ocp4-bm/manifests/cluster-scheduler-02-config.yml
cat ${WORKDIR}/ocp4-bm/manifests/cluster-scheduler-02-config.yml
${WORKDIR}/bin/openshift-install-45 create ignition-configs --dir=ocp4-bm
{% endhighlight %}

 - Run the script
{% highlight bash %}
$ sh -x deploy-45.sh
+ rm -rf ocp4-bm/
+ mkdir ocp4-bm
+ cp install-config-45.yaml ocp4-bm/install-config.yaml
+ ./bin/openshift-install-45 create manifests --dir=ocp4-bm
INFO Consuming Install Config from target directory 
WARNING Making control-plane schedulable by setting MastersSchedulable to true for Scheduler cluster settings 
+ sed -i 's/mastersSchedulable: true/mastersSchedulable: False/' ocp4-bm/manifests/cluster-scheduler-02-config.yml
+ cat ocp4-bm/manifests/cluster-scheduler-02-config.yml
apiVersion: config.openshift.io/v1
kind: Scheduler
metadata:
  creationTimestamp: null
  name: cluster
spec:
  mastersSchedulable: False
  policy:
    name: ""
status: {}
+ ./bin/openshift-install-45 create ignition-configs --dir=ocp4-bm
INFO Consuming Master Machines from target directory 
INFO Consuming Worker Machines from target directory 
INFO Consuming OpenShift Install (Manifests) from target directory 
INFO Consuming Common Manifests from target directory 
INFO Consuming Openshift Manifests from target directory 
{% endhighlight %}

 - After the script runs the directory looks like the following.
{% highlight bash %}
$ tree ocp4-bm
ocp4-bm
├── auth
│   ├── kubeadmin-password
│   └── kubeconfig
├── bootstrap.ign
├── master.ign
├── metadata.json
└── worker.ign

1 directory, 6 files
{% endhighlight %}

{:start="4"}
#### 4. Create the bootstrap and master VMs

Following there is a very simple script to create them. But there are some remarks to do: I am also passing the network configuration for static IPs, and also forcing the MAC Address that I used in the previous section where I created the helper node. If I was using the DHCP configuration the MAC would be important to get the right IP assigned. Here is not really needed but does not harm either.

 - Bootstrap node: 192.168.133.50
 - Master 0 node: 192.168.133.60
 - Master 1 node: 192.168.133.61
 - Master 2 node: 192.168.133.62

{% highlight bash %}
$ cat create_master_vms_45.sh
#!/bin/bash

# Bootstrap
virt-install --name ice-ocp4-bootstrap \
  --vcpus 4 --ram 16384 --cpu host-passthrough,+vmx --disk size=50 \
  --os-type linux --os-variant rhel8.1 \
  --noautoconsole --vnc \
  --network network=ocp4_net,mac=52:54:00:87:84:f4 --noreboot --noautoconsole \
  --location http://192.168.133.1:8080/rhcos/4.5/ \
  --extra-args "nomodeset rd.neednet=1 coreos.inst=yes coreos.inst.install_dev=vda coreos.inst.image_url=http://192.168.133.1:8080/rhcos/4.5/bios.raw.gz coreos.inst.ignition_url=http://192.168.133.1:8080/ocp4-bm/bootstrap.ign ip=192.168.133.50::192.168.133.1:255.255.255.0:ice-ocp4-bootstrap.t1.lab.local:enp1s0:none nameserver=192.168.133.9"

# Masters
for I in 0 1 2; do

	virt-install --name ice-ocp4-master-${I} \
	  --vcpus 4 --ram 16384 --cpu host-passthrough,+vmx --disk size=80 \
	  --os-type linux --os-variant rhel8.1 \
	  --noautoconsole --vnc \
	  --network network=ocp4_net,mac=52:54:00:76:70:0${I} --noreboot --noautoconsole \
	  --location http://192.168.133.1:8080/rhcos/4.5/ \
	  --extra-args "nomodeset rd.neednet=1 coreos.inst=yes coreos.inst.install_dev=vda coreos.inst.image_url=http://192.168.133.1:8080/rhcos/4.5/bios.raw.gz coreos.inst.ignition_url=http://192.168.133.1:8080/ocp4-bm/master.ign ip=192.168.133.6"$(( ${I} + 1))"::192.168.133.1:255.255.255.0:ice-ocp4-master-"${I}".t1.lab.local:enp1s0:none nameserver=192.168.133.9"

done
{% endhighlight %}

 - Run the script to create the VMs
{% highlight bash %}
sh -x create_master_vms_45.sh 
+ virt-install --name ice-ocp4-bootstrap --vcpus 4 --ram 16384 --cpu host-passthrough,+vmx --disk size=50 --os-type linux --os-variant rhel8.1 --noautoconsole --vnc --network network=ocp4_net,mac=52:54:00:87:84:f4 --noreboot --noautoconsole --location http://192.168.133.1:8080/rhcos/4.5/ --extra-args 'nomodeset rd.neednet=1 coreos.inst=yes coreos.inst.install_dev=vda coreos.inst.image_url=http://192.168.133.1:8080/rhcos/4.5/bios.raw.gz coreos.inst.ignition_url=http://192.168.133.1:8080/ocp4-bm/bootstrap.ign ip=192.168.133.50::192.168.133.1:255.255.255.0:ice-ocp4-bootstrap.t1.lab.local:enp1s0:none nameserver=192.168.133.9'

Starting install...
Retrieving file vmlinuz...                                                                              | 8.5 MB  00:00:00     
Retrieving file initramfs.img...                                                                        |  71 MB  00:00:00     
Allocating 'virtinst-k74h0fff-vmlinuz'                                                                  | 8.5 MB  00:00:00     
Transferring virtinst-k74h0fff-vmlinuz                                                                  | 8.5 MB  00:00:00     
Allocating 'virtinst-le2zgtal-initramfs.img'                                                            |  71 MB  00:00:00     
Transferring virtinst-le2zgtal-initramfs.img                                                            |  71 MB  00:00:04     
Allocating 'ice-ocp4-bootstrap.qcow2'                                                                   |  50 GB  00:00:10     
Domain installation still in progress. You can reconnect to 
the console to complete the installation process.
+ for I in 0 1 2
+ virt-install --name ice-ocp4-master-0 --vcpus 4 --ram 16384 --cpu host-passthrough,+vmx --disk size=80 --os-type linux --os-variant rhel8.1 --noautoconsole --vnc --network network=ocp4_net,mac=52:54:00:76:70:00 --noreboot --noautoconsole --location http://192.168.133.1:8080/rhcos/4.5/ --extra-args 'nomodeset rd.neednet=1 coreos.inst=yes coreos.inst.install_dev=vda coreos.inst.image_url=http://192.168.133.1:8080/rhcos/4.5/bios.raw.gz coreos.inst.ignition_url=http://192.168.133.1:8080/ocp4-bm/master.ign ip=192.168.133.61::192.168.133.1:255.255.255.0:ice-ocp4-master-0.t1.lab.local:enp1s0:none nameserver=192.168.133.9'

Starting install...
Retrieving file vmlinuz...                                                                              | 8.5 MB  00:00:00     
...
{% endhighlight %}

 - Meanwhile the python webserver logs the requests being made from the newly created VMs.
{% highlight bash %}
192.168.133.1 - - [28/Aug/2020 15:23:40] "GET /rhcos/4.5/vmlinuz HTTP/1.1" 200 -
192.168.133.1 - - [28/Aug/2020 15:23:40] "GET /rhcos/4.5/initramfs.img HTTP/1.1" 200 -
192.168.133.50 - - [28/Aug/2020 15:23:52] "HEAD /rhcos/4.5/bios.raw.gz HTTP/1.1" 200 -
192.168.133.50 - - [28/Aug/2020 15:23:52] "GET /ocp4-bm/bootstrap.ign HTTP/1.1" 200 -
192.168.133.50 - - [28/Aug/2020 15:23:52] "GET /rhcos/4.5/bios.raw.gz HTTP/1.1" 200 -
192.168.133.1 - - [28/Aug/2020 15:24:18] "GET /rhcos/4.5//.treeinfo HTTP/1.1" 200 -
192.168.133.1 - - [28/Aug/2020 15:24:18] "GET /rhcos/4.5/.treeinfo HTTP/1.1" 200 -
192.168.133.63 - - [28/Aug/2020 15:26:33] "GET /rhcos/4.5/bios.raw.gz HTTP/1.1" 200 -
...
{% endhighlight %}

 - After a while the VMs are going to boot from the network install the OS image and shutdown. Then I will start them manually again this time booting from disk.

{:start="5"}
 5. Wait for the bootstrap to complete
{% highlight bash %}
$ ./bin/openshift-install-45 wait-for bootstrap-complete --dir=ocp4-bm
INFO Waiting up to 20m0s for the Kubernetes API at https://api.ocp4-bm.t1.lab.local:6443... 
INFO API v1.18.3+012b3ec up                       
INFO Waiting up to 40m0s for bootstrapping to complete... 
INFO It is now safe to remove the bootstrap resources 
INFO Time elapsed: 1s        
{% endhighlight %}


{:start="6"}
#### 6. Remove the boostrap node from the load balancer pool and then safely turn off and remove the node.

{% highlight bash %}
[root@ice-ocp4-helpernode ~]# vi /etc/haproxy/haproxy.cfg
[root@ice-ocp4-helpernode ~]# systemctl restart haproxy
{% endhighlight %}

#### 7. Create the worker node VMs

 - Worker 0 node: 192.168.133.70
 - Worker 1 node: 192.168.133.71

 - Similarly to the masters script the following will create the worker nodes. Notice that the indicated ignition config is now __worker.ign__.
{% highlight bash %}
$ cat create_worker_vms_45.sh 
#!/bin/bash

# Workers
for I in 0 1; do
	virt-install --name ice-ocp4-worker-${I} \
      --vcpus 4 --ram 16384 --cpu host-passthrough,+vmx --disk size=80 \
	  --os-type linux --os-variant rhel8.1 \
	  --noautoconsole --vnc \
	  --network network=ocp4_net --noreboot --noautoconsole \
	  --location http://192.168.133.1:8080/rhcos/4.5/ \
	  --extra-args "nomodeset rd.neednet=1 coreos.inst=yes coreos.inst.install_dev=vda coreos.inst.image_url=http://192.168.133.1:8080/rhcos/4.5/bios.raw.gz coreos.inst.ignition_url=http://192.168.133.1:8080/ocp4-bm/worker.ign ip=192.168.133.7"$(( ${I} + 1))"::192.168.133.1:255.255.255.0:ice-ocp4-worker-"${I}".t1.lab.local:enp1s0:none nameserver=192.168.133.9"
done
{% endhighlight %}

 - Run the script
{% highlight bash %}
$ sh create_worker_vms_45.sh 

Starting install...
Retrieving file vmlinuz...                                                    | 8.5 MB  00:00:00
Retrieving file initramfs.img...                                              |  71 MB  00:00:00
Allocating 'virtinst-6gww3ocx-vmlinuz'                                        | 8.5 MB  00:00:00
Transferring virtinst-6gww3ocx-vmlinuz                                        | 8.5 MB  00:00:00
Allocating 'virtinst-r0ewivd2-initramfs.img'                                  |  71 MB  00:00:00
Transferring virtinst-r0ewivd2-initramfs.img                                  |  71 MB  00:00:06
Allocating 'ice-ocp4-worker-0.qcow2'                                          |  80 GB  00:00:34
Domain installation still in progress. You can reconnect to
the console to complete the installation process.

Starting install...
Retrieving file vmlinuz...                                                    | 8.5 MB  00:00:00
Retrieving file initramfs.img...                                              |  71 MB  00:00:00
Allocating 'virtinst-o_24aird-vmlinuz'                                        | 8.5 MB  00:00:00
Transferring virtinst-o_24aird-vmlinuz                                        | 8.5 MB  00:00:00
Allocating 'virtinst-gh6b6uq8-initramfs.img'                                  |  71 MB  00:00:00
Transferring virtinst-gh6b6uq8-initramfs.img                                  |  71 MB  00:00:06
Allocating 'ice-ocp4-worker-1.qcow2'                                          |  80 GB  00:00:33
Domain installation still in progress. You can reconnect to 
the console to complete the installation process.
{% endhighlight %}


#### 8. After a while we will see the certificate signing requests listed in the cluster for the new nodes which need to be approved.
 
{% highlight bash %}
$ oc get nodes
NAME                             STATUS   ROLES    AGE   VERSION
ice-ocp4-master-0.t1.lab.local   Ready    master   9h    v1.18.3+012b3ec
ice-ocp4-master-1.t1.lab.local   Ready    master   9h    v1.18.3+012b3ec
ice-ocp4-master-2.t1.lab.local   Ready    master   9h    v1.18.3+012b3ec

$ oc get csr
NAME        AGE     SIGNERNAME                                    REQUESTOR                                                                   CONDITION
csr-24sr8   16m     kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Pending
csr-cqbx6   18m     kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Pending
csr-gxnw9   33m     kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Pending
csr-vzl94   31m     kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Pending
csr-wvht5   3m29s   kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Pending
csr-x4p9d   89s     kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Pending

$  oc get csr -o go-template='\{\{range .items}\}\{{if not .status\}\}\{\{.metadata.name\}\}\{\{"\n"\}\}\{\{end\}\}\{\{end\}\}' | xargs oc adm certificate approve
certificatesigningrequest.certificates.k8s.io/csr-24sr8 approved
certificatesigningrequest.certificates.k8s.io/csr-cqbx6 approved
certificatesigningrequest.certificates.k8s.io/csr-gxnw9 approved
certificatesigningrequest.certificates.k8s.io/csr-vzl94 approved
certificatesigningrequest.certificates.k8s.io/csr-wvht5 approved
certificatesigningrequest.certificates.k8s.io/csr-x4p9d approved
{% endhighlight %}

 - Now we have the new worker nodes added to the cluster, lets wait for the kubelet to start and finish the set everything up, which will be 5 to 10 minutes.

{% highlight bash %}
$ oc get nodes
NAME                             STATUS     ROLES    AGE     VERSION
ice-ocp4-master-0.t1.lab.local   Ready      master   9h      v1.18.3+012b3ec
ice-ocp4-master-1.t1.lab.local   Ready      master   9h      v1.18.3+012b3ec
ice-ocp4-master-2.t1.lab.local   Ready      master   9h      v1.18.3+012b3ec
ice-ocp4-worker-0.t1.lab.local   NotReady   worker   5m53s   v1.18.3+012b3ec
ice-ocp4-worker-1.t1.lab.local   NotReady   worker   5m53s   v1.18.3+012b3ec
{% endhighlight %}

 - After a while
{% highlight bash %}
$ oc get nodes
NAME                             STATUS   ROLES    AGE   VERSION
ice-ocp4-master-0.t1.lab.local   Ready    master   9h    v1.18.3+012b3ec
ice-ocp4-master-1.t1.lab.local   Ready    master   9h    v1.18.3+012b3ec
ice-ocp4-master-2.t1.lab.local   Ready    master   10h   v1.18.3+012b3ec
ice-ocp4-worker-0.t1.lab.local   Ready    worker   10m   v1.18.3+012b3ec
ice-ocp4-worker-1.t1.lab.local   Ready    worker   10m   v1.18.3+012b3ec
{% endhighlight %}

#### 9. Now we can wait for the installation to complete 

{% highlight bash %}
$ ./bin/openshift-install-45 wait-for install-complete --dir=ocp4-bm
INFO Waiting up to 30m0s for the cluster at https://api.ocp4-bm.t1.lab.local:6443 to initialize... 
W0902 10:13:26.227562  677503 reflector.go:326] k8s.io/client-go/tools/watch/informerwatcher.go:146: watch of *v1.ClusterVersion ended with: very short watch: k8s.io/client-go/tools/watch/informerwatcher.go:146: Unexpected watch close - watch lasted less than a second and no items received
W0902 10:15:55.485583  677503 reflector.go:326] k8s.io/client-go/tools/watch/informerwatcher.go:146: watch of *v1.ClusterVersion ended with: very short watch: k8s.io/client-go/tools/watch/informerwatcher.go:146: Unexpected watch close - watch lasted less than a second and no items received
INFO Waiting up to 10m0s for the openshift-console route to be created... 
INFO Install complete!
INFO To access the cluster as the system:admin user when using 'oc', run 'export KUBECONFIG=/home/maur0x/ocp4-baremetal/ocp4-bm/auth/kubeconfig' 
INFO Access the OpenShift web-console here: https://console-openshift-console.apps.ocp4-bm.t1.lab.local 
INFO Login to the console with user: "kubeadmin", and password: "3cXgS-Mgh8D-kdgs3-cGKXA" 
INFO Time elapsed: 20m56s
{% endhighlight %}

**NOTE:** etcdserver pods use to timeout because the disk I am using for the master vms is quite slow.


 - After installation completes all the operators are up and running.
 
{% highlight bash %}
$ oc get co
NAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE
authentication                             4.5.4     True        False         False      9m39s
cloud-credential                           4.5.4     True        False         False      10h
cluster-autoscaler                         4.5.4     True        False         False      9h
config-operator                            4.5.4     True        False         False      9h
console                                    4.5.4     True        False         False      9m55s
csi-snapshot-controller                    4.5.4     True        False         False      25m
dns                                        4.5.4     True        False         False      10h
etcd                                       4.5.4     True        False         False      10h
image-registry                             4.5.4     True        False         True       10h
ingress                                    4.5.4     True        False         False      25m
insights                                   4.5.4     True        False         False      10h
kube-apiserver                             4.5.4     True        False         False      10h
kube-controller-manager                    4.5.4     True        False         False      10h
kube-scheduler                             4.5.4     True        False         False      10h
kube-storage-version-migrator              4.5.4     True        False         False      32s
machine-api                                4.5.4     True        False         False      10h
machine-approver                           4.5.4     True        False         False      10h
machine-config                             4.5.4     True        False         False      14m
marketplace                                4.5.4     True        False         False      10h
monitoring                                 4.5.4     True        False         False      8m49s
network                                    4.5.4     True        False         True       10h
node-tuning                                4.5.4     True        False         False      10h
openshift-apiserver                        4.5.4     True        False         False      14s
openshift-controller-manager               4.5.4     True        False         False      10h
openshift-samples                          4.5.4     True        False         False      9h
operator-lifecycle-manager                 4.5.4     True        False         False      10h
operator-lifecycle-manager-catalog         4.5.4     True        False         False      10h
operator-lifecycle-manager-packageserver   4.5.4     True        False         False      15m
service-ca                                 4.5.4     True        False         False      10h
storage                                    4.5.4     True        False         False      10h

$ oc cluster-info
Kubernetes master is running at https://api.ocp4-bm.t1.lab.local:6443

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
{% endhighlight %}


### What is next?

We have a new cluster running now, so what can we do next ? 
There is a set of tasks one would normally run right after the installation:

 - Setup authentication with an external identity provider like LDAP or AD
 - Disable the default kubeadmin account
 - Setup storage like OCS / NFS etc.
 - Setup infrastructure nodes to host cluster services' pods
 - Setup the registry internal and/or external
 - Setup NTP
 - Setup custom certificates for the API and ingress
 - Backup etcd
 - Enabling etcd encryption
 - Enabling machine autoscaling
 - Setting up a service mesh
 - Setting up logging and monitoring
 - Setting up your monitoring storage persistent


But let's leave that for another round.

