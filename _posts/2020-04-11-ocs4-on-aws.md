---
layout: posts
title: "OCS4 on AWS"
date: 2020-04-11
categories: [blog]
tags: [ ocs, ocs4, aws, ocp, kubernetes ]
author: mauroseb
excerpt_separator: "<!-- more -->"
---
### Intro

In this post I will cover the installation of OpenShift Container Storage 4.2 from stratch on top of a running OpenShift 4.3 deployed on AWS. In essence the procedure is straightforward, and will consist in creating the storage nodes through machinesets, then deploy the OCS operator and create the Storage Cluster on these new nodes. Then I will create different workloads to consume those storage classes, and finally scale out OCS.

<!-- more -->

### OCS Components

OpenShift Container Storage 4 deploys three operators.

 - **OpenShift Container Storage Operator**. The OpenShift Container Storage operator is the primary operator for OpenShift Container Storage. It serves to facilitate the other operators in OpenShift Container Storage by performing administrative tasks outside their scope as well as watching and configuring their CustomResources. Maintains the StorageCluster CRD.

 - **Rook Operator**. [Rook][^1] deploys and manages Ceph on Kubernetes which provides block and file storage. Maintains the CephCluster CRD. The user should only interact with the StorageCluster and let the Storage Operator 

 - **NooBaa Operator**. The NooBaa operator deploys and manages the [NooBaa][^2] Multi-Cloud Gateway on OpenShift, which provides object storage.

### Environment

There is already a running OpenShift 4 cluster with 3 masters and 2 worker nodes in AWS us-east-1 region. It creates some machinesets created by default.

{% highlight console %}
[maur0x@clientvm 0 ~]$ oc get nodes -l node-role.kubernetes.io/worker
NAME                           STATUS   ROLES    AGE    VERSION
ip-10-0-129-31.ec2.internal    Ready    worker   108m   v1.14.6+70aebec30
ip-10-0-159-132.ec2.internal   Ready    worker   108m   v1.14.6+70aebec30

[maur0x@clientvm 0 ~]$ oc get machinesets -n openshift-machine-api | grep -v infra
NAME                                       DESIRED   CURRENT   READY   AVAILABLE   AGE
cluster-ocs-e388-c84n5-worker-us-east-1a   1         1         1       1           117m
cluster-ocs-e388-c84n5-worker-us-east-1b   1         1         1       1           117m
cluster-ocs-e388-c84n5-worker-us-east-1c   0         0                             117m
cluster-ocs-e388-c84n5-worker-us-east-1d   0         0                             117m
cluster-ocs-e388-c84n5-worker-us-east-1e   0         0                             117m
cluster-ocs-e388-c84n5-worker-us-east-1f   0         0                             117m

{% endhighlight %}


### Procedure

#### 1. Create New MachineSets and OCS Workers

To create new machinesets for the storage nodes we have to re-use the Cluster ID from our cluster and replace it in the machinesets definition template.

{% highlight console %}
[maur0x@clientvm 0 ~]$ CLUSTERID=$(oc get machineset -n openshift-machine-api -o jsonpath='{.items[0].metadata.labels.machine\.openshift\.io/cluster-api-cluster}')
[maur0x@clientvm 0 ~]$ echo $CLUSTERID
cluster-ocs-e388-c84n5

{% endhighlight %}


The template has 3 machinesets one in each availability zone (us-east-1a, us-east-1b and us-east-1c in this case). The definition can be mainly copied from the default worker machinesets. For instance the AMI ID will change in a different region. Machine size for this test is m4.4xlarge. I assigned the role **storage-node** on top of the default **worker** role to the machines that are created from these machinesets and only one replica per zone is being created at deployment time which will yield a three node cluster.

{% highlight console %}
apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: CLUSTERID
    machine.openshift.io/cluster-api-machine-role: workerocs
    machine.openshift.io/cluster-api-machine-type: workerocs
  name: CLUSTERID-workerocs-us-east-1a
  namespace: openshift-machine-api
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: CLUSTERID
      machine.openshift.io/cluster-api-machineset: CLUSTERID-workerocs-us-east-1a
  template:
    metadata:
      creationTimestamp: null
      labels:
        machine.openshift.io/cluster-api-cluster: CLUSTERID
        machine.openshift.io/cluster-api-machine-role: workerocs
        machine.openshift.io/cluster-api-machine-type: workerocs
        machine.openshift.io/cluster-api-machineset: CLUSTERID-workerocs-us-east-1a
    spec:
      metadata:
        creationTimestamp: null
        labels:
          role: storage-node
          node-role.kubernetes.io/worker: ""
      providerSpec:
        value:
          ami:
            id: ami-01e7fdcb66157b224
          apiVersion: awsproviderconfig.openshift.io/v1beta1
          blockDevices:
          - ebs:
              iops: 0
              volumeSize: 120
              volumeType: gp2
          credentialsSecret:
            name: aws-cloud-credentials
          deviceIndex: 0
          iamInstanceProfile:
            id: CLUSTERID-worker-profile
          instanceType: m4.4xlarge
          kind: AWSMachineProviderConfig
          metadata:
            creationTimestamp: null
          placement:
            availabilityZone: us-east-1a
            region: us-east-1
          publicIp: null
          securityGroups:
          - filters:
            - name: tag:Name
              values:
              - CLUSTERID-worker-sg
          subnet:
            filters:
            - name: tag:Name
              values:
              - CLUSTERID-private-us-east-1a
          tags:
          - name: kubernetes.io/cluster/CLUSTERID
            value: owned
          userDataSecret:
            name: worker-user-data
      versions:
        kubelet: ""
---
apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: CLUSTERID
    machine.openshift.io/cluster-api-machine-role: workerocs
    machine.openshift.io/cluster-api-machine-type: workerocs
  name: CLUSTERID-workerocs-us-east-1b
  namespace: openshift-machine-api
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: CLUSTERID
      machine.openshift.io/cluster-api-machineset: CLUSTERID-workerocs-us-east-1b
  template:
    metadata:
      creationTimestamp: null
      labels:
        machine.openshift.io/cluster-api-cluster: CLUSTERID
        machine.openshift.io/cluster-api-machine-role: workerocs
        machine.openshift.io/cluster-api-machine-type: workerocs
        machine.openshift.io/cluster-api-machineset: CLUSTERID-workerocs-us-east-1b
    spec:
      metadata:
        creationTimestamp: null
        labels:
          role: storage-node
          node-role.kubernetes.io/worker: ""
      providerSpec:
        value:
          ami:
            id: ami-01e7fdcb66157b224
          apiVersion: awsproviderconfig.openshift.io/v1beta1
          blockDevices:
          - ebs:
              iops: 0
              volumeSize: 120
              volumeType: gp2
          credentialsSecret:
            name: aws-cloud-credentials
          deviceIndex: 0
          iamInstanceProfile:
            id: CLUSTERID-worker-profile
          instanceType: m4.4xlarge
          kind: AWSMachineProviderConfig
          metadata:
            creationTimestamp: null
          placement:
            availabilityZone: us-east-1b
            region: us-east-1
          publicIp: null
          securityGroups:
          - filters:
            - name: tag:Name
              values:
              - CLUSTERID-worker-sg
          subnet:
            filters:
            - name: tag:Name
              values:
              - CLUSTERID-private-us-east-1b
          tags:
          - name: kubernetes.io/cluster/CLUSTERID
            value: owned
          userDataSecret:
            name: worker-user-data
      versions:
        kubelet: ""
---
apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: CLUSTERID
    machine.openshift.io/cluster-api-machine-role: workerocs
    machine.openshift.io/cluster-api-machine-type: workerocs
  name: CLUSTERID-workerocs-us-east-1c
  namespace: openshift-machine-api
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: CLUSTERID
      machine.openshift.io/cluster-api-machineset: CLUSTERID-workerocs-us-east-1c
  template:
    metadata:
      creationTimestamp: null
      labels:
        machine.openshift.io/cluster-api-cluster: CLUSTERID
        machine.openshift.io/cluster-api-machine-role: workerocs
        machine.openshift.io/cluster-api-machine-type: workerocs
        machine.openshift.io/cluster-api-machineset: CLUSTERID-workerocs-us-east-1c
    spec:
      metadata:
        creationTimestamp: null
        labels:
          role: storage-node
          node-role.kubernetes.io/worker: ""
      providerSpec:
        value:
          ami:
            id: ami-01e7fdcb66157b224
          apiVersion: awsproviderconfig.openshift.io/v1beta1
          blockDevices:
          - ebs:
              iops: 0
              volumeSize: 120
              volumeType: gp2
          credentialsSecret:
            name: aws-cloud-credentials
          deviceIndex: 0
          iamInstanceProfile:
            id: CLUSTERID-worker-profile
          instanceType: m4.4xlarge
          kind: AWSMachineProviderConfig
          metadata:
            creationTimestamp: null
          placement:
            availabilityZone: us-east-1c
            region: us-east-1
          publicIp: null
          securityGroups:
          - filters:
            - name: tag:Name
              values:
              - CLUSTERID-worker-sg
          subnet:
            filters:
            - name: tag:Name
              values:
              - CLUSTERID-private-us-east-1c
          tags:
          - name: kubernetes.io/cluster/CLUSTERID
            value: owned
          userDataSecret:
            name: worker-user-data
      versions:
        kubelet: ""
{% endhighlight %}


Now by creating the machinesets 3 new nodes will be added automatically.

{% highlight console %}
[maur0x@clientvm 0 ~]$ cat cluster-workerocs.yaml | sed -e "s/CLUSTERID/${CLUSTERID}/g" | oc apply -f -
machineset.machine.openshift.io/cluster-ocs-e388-c84n5-workerocs-us-east-1a created
machineset.machine.openshift.io/cluster-ocs-e388-c84n5-workerocs-us-east-1b created
machineset.machine.openshift.io/cluster-ocs-e388-c84n5-workerocs-us-east-1c created

[maur0x@clientvm 0 ~]$ oc get machines -n openshift-machine-api | egrep 'NAME|workerocs'
NAME                                                STATE     TYPE         REGION      ZONE         AGE
cluster-ocs-e388-c84n5-workerocs-us-east-1a-97px4   running   m4.4xlarge   us-east-1   us-east-1a   2m11s
cluster-ocs-e388-c84n5-workerocs-us-east-1b-92xt7   running   m4.4xlarge   us-east-1   us-east-1b   2m11s
cluster-ocs-e388-c84n5-workerocs-us-east-1c-7n4s4   running   m4.4xlarge   us-east-1   us-east-1c   2m11s
{% endhighlight %}


Since they are also tagged as worker nodes they can be listed along with the original two workers.

{% highlight console %}
[maur0x@clientvm 0 ~]$ oc get nodes -l node-role.kubernetes.io/worker
NAME                           STATUS   ROLES    AGE    VERSION
ip-10-0-129-31.ec2.internal    Ready    worker   147m   v1.14.6+70aebec30
ip-10-0-135-241.ec2.internal   Ready    worker   31m    v1.14.6+70aebec30
ip-10-0-146-22.ec2.internal    Ready    worker   30m    v1.14.6+70aebec30
ip-10-0-159-132.ec2.internal   Ready    worker   147m   v1.14.6+70aebec30
ip-10-0-162-88.ec2.internal    Ready    worker   31m    v1.14.6+70aebec30
{% endhighlight %}


### 2. Create OpenShift Container Storage Namespace and OperatorGroup

OpenShift Container Storage runs only in the openshift-storage namespace, which needs to be created before subscription. The following manifest can be used to create the namespace.

{% highlight console %}
apiVersion: v1
kind: Namespace
metadata:
  labels:
    openshift.io/cluster-monitoring: "true"
  name: openshift-storage
spec: {}
{% endhighlight %}


The namespace will be created by running:
{% highlight console %}
$ oc create -f ocs-namespace.yaml
{% endhighlight %}

Likewise an OperatorGroup targetting the openshift-storage namespace also needs to be created. The following manifest can be used to create the OperatorGroup.

{% highlight console %}
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: openshift-storage-operatorgroup
  namespace: openshift-storage
spec:
  serviceAccount:
    metadata:
      creationTimestamp: null
  targetNamespaces:
  - openshift-storage
{% endhighlight %}

Create the operator group.

{% highlight console %}
$ oc create -f ocs-operatorgroup.yaml
{% endhighlight %}


#### 3. Install in the Red Hat Storage Operator from OperatorHub

In the OpenShift console just navigate to the OperatorHub and search for "OpenShift Container Storage" and install with the defaults and passing the new namespace.
After a while we can see that the installation succeeded and the operator pods are created.

{% highlight console %}
[maur0x@clientvm 0 ~]$ oc -n openshift-storage get csv
NAME                  DISPLAY                       VERSION   REPLACES   PHASE
ocs-operator.v4.2.3   OpenShift Container Storage   4.2.3                Succeeded


[maur0x@clientvm 0 ~]$ oc -n openshift-storage get pods
NAME                                 READY   STATUS    RESTARTS   AGE
noobaa-operator-6d7bdb675-dckgk      1/1     Running   0          59m
ocs-operator-7d96c577f7-f4b4t        1/1     Running   0          59m
rook-ceph-operator-bcc7f79ff-v4cxj   1/1     Running   0          59m


[maur0x@clientvm 130 ~]$ oc -n openshift-storage get pods -w
NAME                                           READY   STATUS              RESTARTS   AGE
csi-cephfsplugin-4jcwk                         3/3     Running             0          22s
csi-cephfsplugin-n5hfv                         0/3     ContainerCreating   0          22s
csi-cephfsplugin-provisioner-6499769bd-bfspp   0/4     ContainerCreating   0          22s
csi-cephfsplugin-provisioner-6499769bd-n7hfp   0/4     ContainerCreating   0          22s
csi-cephfsplugin-qp2pl                         0/3     ContainerCreating   0          22s
csi-cephfsplugin-x8lhw                         0/3     ContainerCreating   0          22s
csi-cephfsplugin-xgjhg                         0/3     ContainerCreating   0          22s
csi-rbdplugin-ctc7d                            0/3     ContainerCreating   0          22s
csi-rbdplugin-d9k4t                            0/3     ContainerCreating   0          22s
csi-rbdplugin-gsc27                            3/3     Running             0          22s
...
{% endhighlight %}


Also make we double check the nodes have the **storage-node** role label in the nodes as we will create the new Storage Cluster.

{% highlight console %}
[maur0x@clientvm 0 ~]$ oc get nodes --show-labels | grep storage-node |cut -d' ' -f1
ip-10-0-135-241.ec2.internal
ip-10-0-146-22.ec2.internal
ip-10-0-162-88.ec2.internal
{% endhighlight %}

After the three operators have been deployed into the openshift-storage namespace, a StorageCluster can be created. Note that the StorageCluster resource is the only resource that a user should be creating. OpenShift Container Storage includes many other custom resources which are internal and not meant for direct usage by users.


#### 3. Create Storage Cluster

In the OCS Operator tab create a new storage cluster indicating the newly created nodes, that will kick off in the background the Rook operator and will create a new Ceph cluster. Afte some time we can check that there are some newly created Storage Classes available from OCS.


{% highlight console %}
[maur0x@clientvm 0 ~]$ oc -n openshift-storage get sc
NAME                          PROVISIONER                             AGE
gp2 (default)                 kubernetes.io/aws-ebs                   4h59m
ocs-storagecluster-ceph-rbd   openshift-storage.rbd.csi.ceph.com      54m
ocs-storagecluster-cephfs     openshift-storage.cephfs.csi.ceph.com   54m
openshift-storage.noobaa.io   openshift-storage.noobaa.io/obc         48m

[maur0x@clientvm 0 ~]$ oc get storagecluster,cephcluster,noobaa -n openshift-storage
NAME                                                 AGE
storagecluster.ocs.openshift.io/ocs-storagecluster   4h52m

NAME                                                      DATADIRHOSTPATH   MONCOUNT   AGE     STATE     HEALTH
cephcluster.ceph.rook.io/ocs-storagecluster-cephcluster   /var/lib/rook     3          4h52m   Created   HEALTH_OK

NAME                      MGMT-ENDPOINTS                 S3-ENDPOINTS                   IMAGE                                                                                                            PHASE   AGE
noobaa.noobaa.io/noobaa   [https://10.0.135.241:32402]   [https://10.0.135.241:30936]   registry.redhat.io/ocs4/mcg-core-rhel8@sha256:08866178f34a93b0f6a3e99aa6127d11b29bd65900f3e30fcd119b354b65fe0d   Ready   4h48m


[maur0x@clientvm 0 ~]$ oc -n openshift-storage get subs,csv,pods
NAME                                             PACKAGE        SOURCE             CHANNEL
subscription.operators.coreos.com/ocs-operator   ocs-operator   redhat-operators   stable-4.2

NAME                                                             DISPLAY                       VERSION   REPLACES   PHASE
clusterserviceversion.operators.coreos.com/ocs-operator.v4.2.3   OpenShift Container Storage   4.2.3                Succeeded

NAME                                                                  READY   STATUS      RESTARTS   AGE
pod/csi-cephfsplugin-4jcwk                                            3/3     Running     0          5h42m
pod/csi-cephfsplugin-n5hfv                                            3/3     Running     0          5h42m
pod/csi-cephfsplugin-provisioner-6499769bd-bfspp                      4/4     Running     0          5h42m
pod/csi-cephfsplugin-provisioner-6499769bd-n7hfp                      4/4     Running     2          5h42m
pod/csi-cephfsplugin-qp2pl                                            3/3     Running     3          5h42m
pod/csi-cephfsplugin-x8lhw                                            3/3     Running     0          5h42m
pod/csi-cephfsplugin-xgjhg                                            3/3     Running     3          5h42m
pod/csi-rbdplugin-ctc7d                                               3/3     Running     3          5h42m
pod/csi-rbdplugin-d9k4t                                               3/3     Running     0          5h42m
pod/csi-rbdplugin-gsc27                                               3/3     Running     0          5h42m
pod/csi-rbdplugin-hgm8r                                               3/3     Running     3          5h42m
pod/csi-rbdplugin-hv2p9                                               3/3     Running     0          5h42m
pod/csi-rbdplugin-provisioner-77f4459cbf-lvqhv                        4/4     Running     0          5h42m
pod/csi-rbdplugin-provisioner-77f4459cbf-pbk8w                        4/4     Running     15         5h42m
pod/noobaa-core-0                                                     2/2     Running     0          5h38m
pod/noobaa-operator-6d7bdb675-dckgk                                   1/1     Running     13         6h44m
pod/ocs-operator-7d96c577f7-f4b4t                                     1/1     Running     0          6h44m
pod/rook-ceph-drain-canary-ip-10-0-135-241.ec2.internal-6947b5xx2qd   1/1     Running     0          5h38m
pod/rook-ceph-drain-canary-ip-10-0-146-22.ec2.internal-84c445fw5nqt   1/1     Running     0          5h38m
pod/rook-ceph-drain-canary-ip-10-0-162-88.ec2.internal-559bb8fhv72j   1/1     Running     0          5h38m
pod/rook-ceph-mds-ocs-storagecluster-cephfilesystem-a-b6f84749x7gmb   1/1     Running     0          5h38m
pod/rook-ceph-mds-ocs-storagecluster-cephfilesystem-b-678bbfb54j285   1/1     Running     0          5h38m
pod/rook-ceph-mgr-a-846d678998-kvq4k                                  1/1     Running     0          5h39m
pod/rook-ceph-mon-a-567d85c4d9-sqws4                                  1/1     Running     0          5h41m
pod/rook-ceph-mon-b-cb99558c4-cj5gp                                   1/1     Running     0          5h40m
pod/rook-ceph-mon-c-b69cbbb99-62jxt                                   1/1     Running     0          5h40m
pod/rook-ceph-operator-bcc7f79ff-v4cxj                                1/1     Running     14         6h44m
pod/rook-ceph-osd-0-77c5cccc9-8b65x                                   1/1     Running     0          5h38m
pod/rook-ceph-osd-1-78b9f54d65-hnb6v                                  1/1     Running     0          5h38m
pod/rook-ceph-osd-2-5674b7768f-gzm6h                                  1/1     Running     0          5h38m
pod/rook-ceph-osd-prepare-ocs-deviceset-0-0-42759-9vl64               0/1     Completed   0          5h39m
pod/rook-ceph-osd-prepare-ocs-deviceset-1-0-t4ks6-4gdr5               0/1     Completed   0          5h39m
pod/rook-ceph-osd-prepare-ocs-deviceset-2-0-cbwf6-k9c5s               0/1     Completed   0          5h39m


[maur0x@clientvm 0 ~]$ oc describe OCSInitialization  -n openshift-storage
Name:         ocsinit
Namespace:    openshift-storage
Labels:       <none>
Annotations:  <none>
API Version:  ocs.openshift.io/v1
Kind:         OCSInitialization
Metadata:
  Creation Timestamp:  2020-04-08T10:33:59Z
  Generation:          2
  Resource Version:    181405
  Self Link:           /apis/ocs.openshift.io/v1/namespaces/openshift-storage/ocsinitializations/ocsinit
  UID:                 7551b5ec-7984-11ea-83d2-024806cffca5
Spec:
  Enable Ceph Tools:  true
Status:
  Conditions:
    Last Heartbeat Time:   2020-04-08T16:03:07Z
    Last Transition Time:  2020-04-08T10:34:00Z
    Message:               Reconcile completed successfully
    Reason:                ReconcileCompleted
    Status:                True
    Type:                  ReconcileComplete
    Last Heartbeat Time:   2020-04-08T16:03:07Z
    Last Transition Time:  2020-04-08T10:34:00Z
    Message:               Reconcile completed successfully
    Reason:                ReconcileCompleted
    Status:                True
    Type:                  Available
    Last Heartbeat Time:   2020-04-08T16:03:07Z
    Last Transition Time:  2020-04-08T10:34:00Z
    Message:               Reconcile completed successfully
    Reason:                ReconcileCompleted
    Status:                False
    Type:                  Progressing
    Last Heartbeat Time:   2020-04-08T16:03:07Z
    Last Transition Time:  2020-04-08T10:34:00Z
    Message:               Reconcile completed successfully
    Reason:                ReconcileCompleted
    Status:                False
    Type:                  Degraded
    Last Heartbeat Time:   2020-04-08T16:03:07Z
    Last Transition Time:  2020-04-08T10:34:00Z
    Message:               Reconcile completed successfully
    Reason:                ReconcileCompleted
    Status:                True
    Type:                  Upgradeable
  Phase:                   Ready
  S C Cs Created:          true
Events:                    <none>
{% endhighlight %}


#### 4. Set RBD as default storage class

Now I will set the default Storage Class to the OCS RBD one (RWO) instead of the AWS default GP2.

{% highlight console %}
[maur0x@clientvm 0 ~]$ oc patch storageclass gp2 -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"false"}}}'
storageclass.storage.k8s.io/gp2 patched
[maur0x@clientvm 0 ~]$ oc patch storageclass ocs-storagecluster-ceph-rbd -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
storageclass.storage.k8s.io/ocs-storagecluster-ceph-rbd patched
{% endhighlight %}


#### 5. Explore deployment

We can see that as part of the process some persistent volumes were created and claimed for the MONs' monstore and the OSDs' disks. Finally Nooba (MCG) will create a volume from the OCS RBD class for its DB.

{% highlight console %}
[maur0x@clientvm 0 ~]$ oc get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                                       STORAGECLASS                  REASON   AGE
pvc-06ab5166-796e-11ea-9045-0ac586f72e0b   1Gi        RWO            Delete           Bound    terminal/terminal-hub-data                  gp2                                    10h
pvc-28fc19ba-798d-11ea-8cc6-1277afaa9b89   10Gi       RWO            Delete           Bound    openshift-storage/rook-ceph-mon-a           gp2                                    6h28m
pvc-2c54790e-798d-11ea-8cc6-1277afaa9b89   10Gi       RWO            Delete           Bound    openshift-storage/rook-ceph-mon-b           gp2                                    6h28m
pvc-2f5408b7-798d-11ea-8cc6-1277afaa9b89   10Gi       RWO            Delete           Bound    openshift-storage/rook-ceph-mon-c           gp2                                    6h28m
pvc-88e0724a-798d-11ea-8cc6-1277afaa9b89   2Ti        RWO            Delete           Bound    openshift-storage/ocs-deviceset-0-0-42759   gp2                                    6h26m
pvc-88e18c3e-798d-11ea-8cc6-1277afaa9b89   2Ti        RWO            Delete           Bound    openshift-storage/ocs-deviceset-1-0-t4ks6   gp2                                    6h26m
pvc-88e2a55a-798d-11ea-8cc6-1277afaa9b89   2Ti        RWO            Delete           Bound    openshift-storage/ocs-deviceset-2-0-cbwf6   gp2                                    6h26m
pvc-a54f25a0-798d-11ea-83d2-024806cffca5   50Gi       RWO            Delete           Bound    openshift-storage/db-noobaa-core-0          ocs-storagecluster-ceph-rbd            6h25m

[maur0x@clientvm 0 ~]$ oc get pvc --all-namespaces
NAMESPACE           NAME                      STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS                  AGE
openshift-storage   db-noobaa-core-0          Bound    pvc-a54f25a0-798d-11ea-83d2-024806cffca5   50Gi       RWO            ocs-storagecluster-ceph-rbd   6h25m
openshift-storage   ocs-deviceset-0-0-42759   Bound    pvc-88e0724a-798d-11ea-8cc6-1277afaa9b89   2Ti        RWO            gp2                           6h26m
openshift-storage   ocs-deviceset-1-0-t4ks6   Bound    pvc-88e18c3e-798d-11ea-8cc6-1277afaa9b89   2Ti        RWO            gp2                           6h26m
openshift-storage   ocs-deviceset-2-0-cbwf6   Bound    pvc-88e2a55a-798d-11ea-8cc6-1277afaa9b89   2Ti        RWO            gp2                           6h26m
openshift-storage   rook-ceph-mon-a           Bound    pvc-28fc19ba-798d-11ea-8cc6-1277afaa9b89   10Gi       RWO            gp2                           6h29m
openshift-storage   rook-ceph-mon-b           Bound    pvc-2c54790e-798d-11ea-8cc6-1277afaa9b89   10Gi       RWO            gp2                           6h29m
openshift-storage   rook-ceph-mon-c           Bound    pvc-2f5408b7-798d-11ea-8cc6-1277afaa9b89   10Gi       RWO            gp2                           6h29m
terminal            terminal-hub-data         Bound    pvc-06ab5166-796e-11ea-9045-0ac586f72e0b   1Gi        RWO            gp2                           10h
{% endhighlight %}

Check the CephCluster resource.
{% highlight console %}
[maur0x@clientvm 1 ~]$ oc get cephcluster -n openshift-storage
NAME                             DATADIRHOSTPATH   MONCOUNT   AGE    STATE     HEALTH
ocs-storagecluster-cephcluster   /var/lib/rook     3          2d4h   Created   HEALTH_OK
{% endhighlight %}

Lets check Ceph status from the operator pod.

{% highlight console %}
[maur0x@clientvm 0 ~]$ oc rsh rook-ceph-operator-bcc7f79ff-v4cxj
sh-4.4$ cp /var/lib/rook/openshift-storage/openshift-storage.config /etc/ceph/ceph.conf
sh-4.4$ ceph df
RAW STORAGE:
    CLASS     SIZE        AVAIL       USED        RAW USED     %RAW USED
    ssd       6.0 TiB     6.0 TiB     2.9 GiB      5.9 GiB          0.10
    TOTAL     6.0 TiB     6.0 TiB     2.9 GiB      5.9 GiB          0.10

POOLS:
    POOL                                           ID     STORED      OBJECTS     USED        %USED     MAX AVAIL
    ocs-storagecluster-cephblockpool                1     981 MiB         519     2.9 GiB      0.05       1.9 TiB
    ocs-storagecluster-cephfilesystem-metadata      2      34 KiB          24     480 KiB         0       1.9 TiB
    ocs-storagecluster-cephfilesystem-data0         3     1.3 MiB           1     4.0 MiB         0       1.9 TiB
sh-4.4$ ceph osd df tree
ID  CLASS WEIGHT  REWEIGHT SIZE    RAW USE DATA    OMAP    META     AVAIL   %USE VAR  PGS STATUS TYPE NAME
 -1       5.99698        - 6.0 TiB 5.9 GiB 2.9 GiB 151 KiB  3.0 GiB 6.0 TiB 0.10 1.00   -        root default
 -5       5.99698        - 6.0 TiB 5.9 GiB 2.9 GiB 151 KiB  3.0 GiB 6.0 TiB 0.10 1.00   -            region us-east-1
-10       1.99899        - 2.0 TiB 2.0 GiB 986 MiB  44 KiB 1024 MiB 2.0 TiB 0.10 1.00   -                zone us-east-1a
 -9       1.99899        - 2.0 TiB 2.0 GiB 986 MiB  44 KiB 1024 MiB 2.0 TiB 0.10 1.00   -                    host ocs-deviceset-0-0-42759
  1   ssd 1.99899  1.00000 2.0 TiB 2.0 GiB 986 MiB  44 KiB 1024 MiB 2.0 TiB 0.10 1.00  24     up                 osd.1
-14       1.99899        - 2.0 TiB 2.0 GiB 986 MiB  55 KiB 1024 MiB 2.0 TiB 0.10 1.00   -                zone us-east-1b
-13       1.99899        - 2.0 TiB 2.0 GiB 986 MiB  55 KiB 1024 MiB 2.0 TiB 0.10 1.00   -                    host ocs-deviceset-2-0-cbwf6
  2   ssd 1.99899  1.00000 2.0 TiB 2.0 GiB 986 MiB  55 KiB 1024 MiB 2.0 TiB 0.10 1.00  24     up                 osd.2
 -4       1.99899        - 2.0 TiB 2.0 GiB 986 MiB  52 KiB 1024 MiB 2.0 TiB 0.10 1.00   -                zone us-east-1c
 -3       1.99899        - 2.0 TiB 2.0 GiB 986 MiB  52 KiB 1024 MiB 2.0 TiB 0.10 1.00   -                    host ocs-deviceset-1-0-t4ks6
  0   ssd 1.99899  1.00000 2.0 TiB 2.0 GiB 986 MiB  52 KiB 1024 MiB 2.0 TiB 0.10 1.00  24     up                 osd.0
                     TOTAL 6.0 TiB 5.9 GiB 2.9 GiB 152 KiB  3.0 GiB 6.0 TiB 0.10
MIN/MAX VAR: 1.00/1.00  STDDEV: 0

sh-4.4$ ceph -s
  cluster:
    id:     736b621b-c423-49c7-a664-44f54e2134cd
    health: HEALTH_OK

  services:
    mon: 3 daemons, quorum a,b,c (age 2d)
    mgr: a(active, since 2d)
    mds: ocs-storagecluster-cephfilesystem:1 {0=ocs-storagecluster-cephfilesystem-b=up:active} 1 up:standby-replay
    osd: 3 osds: 3 up (since 2d), 3 in (since 2d)

  data:
    pools:   3 pools, 24 pgs
    objects: 546 objects, 1.3 GiB
    usage:   5.9 GiB used, 6.0 TiB / 6.0 TiB avail
    pgs:     24 active+clean

  io:
    client:   1.2 KiB/s rd, 406 KiB/s wr, 2 op/s rd, 2 op/s wr

sh-4.4$ ceph osd pool ls
ocs-storagecluster-cephblockpool
ocs-storagecluster-cephfilesystem-metadata
ocs-storagecluster-cephfilesystem-data0
sh-4.4$ exit

{% endhighlight %}

Similarly one could start the toolbox pod instead of using the operator pod.

{% highlight console %}
[maur0x@clientvm 0 ~]$ TOOLS_POD=$(oc get pods -n openshift-storage -l app=rook-ceph-tools -o name)
[maur0x@clientvm 0 ~]$ oc rsh -n openshift-storage $TOOLS_POD ceph -s
{% endhighlight %}


### Test RWO storage with a test project

By creating a test project that uses a persistent volume, I will test the OCS RBD storage class.

{% highlight console %}
[maur0x@clientvm 1 ~]$ oc new-project my-database-app
curl https://raw.githubusercontent.com/red-hat-storage/ocs-training/master/ocp4ocs4/configurable-rails-app.yaml | oc new-app -p STORAGE_CLASS=ocs-storagecluster-ceph-rbd -p VOLUME_CAPACITY=5Gi -f -

[maur0x@clientvm 127 ~]$ oc get pods
NAME                             READY   STATUS      RESTARTS   AGE
postgresql-1-deploy              0/1     Completed   0          102s
postgresql-1-wjxjq               1/1     Running     0          93s
rails-pgsql-persistent-1-build   1/1     Running     0          103s
[maur0x@clientvm 0 ~]$ oc status
In project my-database-app on server https://api.cluster-ocs-e388.ocs-e388.example.opentlc.com:6443

svc/postgresql - 172.30.148.177:5432
  dc/postgresql deploys openshift/postgresql:10
    deployment #1 deployed 21 minutes ago - 1 pod

http://rails-pgsql-persistent-my-database-app.apps.cluster-ocs-e388.ocs-e388.example.opentlc.com (svc/rails-pgsql-persistent)
  dc/rails-pgsql-persistent deploys istag/rails-pgsql-persistent:latest <-
    bc/rails-pgsql-persistent source builds https://github.com/sclorg/rails-ex.git on openshift/ruby:2.5
    deployment #1 deployed 18 minutes ago - 1 pod


3 infos identified, use 'oc status --suggest' to see details.
{% endhighlight %}


### Test RWX storage with a test project

Now lets quickly create a test project to consume RWX volumes and use the Ceph FS class.

{% highlight console %}
[maur0x@clientvm 1 ~]$ oc new-project my-shared-storage
Now using project "my-shared-storage" on server "https://api.cluster-ocs-e388.ocs-e388.example.opentlc.com:6443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app django-psql-example

to build a new example application in Python. Or use kubectl to deploy a simple Kubernetes application:

    kubectl create deployment hello-node --image=gcr.io/hello-minikube-zero-install/hello-node

[maur0x@clientvm 0 ~]$ oc new-app openshift/php:7.1~https://github.com/christianh814/openshift-php-upload-demo --name=file-uploader
--> Found image 8e01e80 (4 months old) in image stream "openshift/php" under tag "7.1" for "openshift/php:7.1"

    Apache 2.4 with PHP 7.1
    -----------------------
    PHP 7.1 available as container is a base platform for building and running various PHP 7.1 applications and frameworks. PHP is an HTML-embedded scripting language. PHP attempts to make it easy for developers to write dynamically generated web pages. PHP also offers built-in database integration for several commercial and non-commercial database management systems, so writing a database-enabled webpage with PHP is fairly simple. The most common use of PHP coding is probably as a replacement for CGI scripts.

    Tags: builder, php, php71, rh-php71

    * A source build using source code from https://github.com/christianh814/openshift-php-upload-demo will be created
      * The resulting image will be pushed to image stream tag "file-uploader:latest"
      * Use 'oc start-build' to trigger a new build
    * This image will be deployed in deployment config "file-uploader"
    * Ports 8080/tcp, 8443/tcp will be load balanced by service "file-uploader"
      * Other containers can access this service through the hostname "file-uploader"

--> Creating resources ...
    imagestream.image.openshift.io "file-uploader" created
    buildconfig.build.openshift.io "file-uploader" created
    deploymentconfig.apps.openshift.io "file-uploader" created
    service "file-uploader" created
--> Success
    Build scheduled, use 'oc logs -f bc/file-uploader' to track its progress.
    Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
     'oc expose svc/file-uploader'
    Run 'oc status' to view your app.

[maur0x@clientvm 0 ~]$ oc get bc
NAME            TYPE     FROM   LATEST
file-uploader   Source   Git    1

[maur0x@clientvm 0 ~]$ oc expose svc/file-uploader -n my-shared-storage
route.route.openshift.io/file-uploader exposed

[maur0x@clientvm 0 ~]$ oc get route -n my-shared-storage
NAME            HOST/PORT                                                                            PATH   SERVICES        PORT       TERMINATION   WILDCARD
file-uploader   file-uploader-my-shared-storage.apps.cluster-ocs-e388.ocs-e388.example.opentlc.com          file-uploader   8080-tcp                 None
[maur0x@clientvm 0 ~]$ oc scale --replicas=3 dc/file-uploader -n my-shared-storage
deploymentconfig.apps.openshift.io/file-uploader scaled
[maur0x@clientvm 0 ~]$ oc get pods -n my-shared-storage
NAME                     READY   STATUS      RESTARTS   AGE
file-uploader-1-2fs5h    1/1     Running     0          33s
file-uploader-1-build    0/1     Completed   0          109s
file-uploader-1-deploy   0/1     Completed   0          41s

[maur0x@clientvm 0 ~]$ oc get pods -n my-shared-storage
NAME                     READY   STATUS      RESTARTS   AGE
file-uploader-1-2fs5h    1/1     Running     0          69s
file-uploader-1-7df8m    1/1     Running     0          36s
file-uploader-1-build    0/1     Completed   0          2m25s
file-uploader-1-deploy   0/1     Completed   0          77s
file-uploader-1-kbtxm    1/1     Running     0          36s

[maur0x@clientvm 0 ~]$ oc set volume dc/file-uploader --add --name=my-shared-storage \
> -t pvc --claim-mode=ReadWriteMany --claim-size=1Gi \
> --claim-name=my-shared-storage --claim-class=ocs-storagecluster-cephfs \
> --mount-path=/opt/app-root/src/uploaded \
> -n my-shared-storage
deploymentconfig.apps.openshift.io/file-uploader volume updated
[maur0x@clientvm 0 ~]$ oc get pvc -n my-shared-storage
NAME                STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS                AGE
my-shared-storage   Bound    pvc-239eaeb6-7a99-11ea-9f09-024806cffca5   1Gi        RWX            ocs-storagecluster-cephfs   32s

[maur0x@clientvm 1 ~]$ oc get route file-uploader -n my-shared-storage -o jsonpath --template="{.spec.host}{'\n'}"
file-uploader-my-shared-storage.apps.cluster-ocs-e388.ocs-e388.example.opentlc.com

{% endhighlight %}


#### Test Nooba Object Bucket claim

The OBC allows object storage (S3 or RGW like) to be consume in a similar fashion to a PVC.

First download Nooba CLI.
{% highlight console %}
[maur0x@clientvm 127 ~]$ curl -s https://api.github.com/repos/noobaa/noobaa-operator/releases/latest | grep "linux" | cut -d : -f 2,3 | tr -d \" | wget -qi - ; mv noobaa-linux-* noobaa ; chmod +x noobaa; sudo mv noobaa /usr/bin/
{% endhighlight %}


Use the Nooba CLI to check Nooba status

{% highlight console %}
maur0x@clientvm 0 ~]$ noobaa status -n openshift-storage
INFO[0000] CLI version: 2.1.0
INFO[0000] noobaa-image: noobaa/noobaa-core:5.3.0
INFO[0000] operator-image: noobaa/noobaa-operator:2.1.0
INFO[0000] Namespace: openshift-storage
INFO[0000]
INFO[0000] CRD Status:
INFO[0000] ✅ Exists: CustomResourceDefinition "noobaas.noobaa.io"
INFO[0000] ✅ Exists: CustomResourceDefinition "backingstores.noobaa.io"
INFO[0000] ✅ Exists: CustomResourceDefinition "bucketclasses.noobaa.io"
INFO[0000] ✅ Exists: CustomResourceDefinition "objectbucketclaims.objectbucket.io"
INFO[0000] ✅ Exists: CustomResourceDefinition "objectbuckets.objectbucket.io"
INFO[0000]
INFO[0000] Operator Status:
INFO[0000] ✅ Exists: Namespace "openshift-storage"
INFO[0000] ✅ Exists: ServiceAccount "noobaa"
INFO[0000] ✅ Exists: Role "ocs-operator.v4.2.3-97rdt"
INFO[0000] ✅ Exists: RoleBinding "ocs-operator.v4.2.3-97rdt-noobaa-9szs6"
INFO[0000] ✅ Exists: ClusterRole "ocs-operator.v4.2.3-ftg2p"
INFO[0000] ✅ Exists: ClusterRoleBinding "ocs-operator.v4.2.3-ftg2p-noobaa-bpv7r"
INFO[0000] ✅ Exists: Deployment "noobaa-operator"
INFO[0000]
INFO[0000] System Status:
INFO[0000] ✅ Exists: NooBaa "noobaa"
INFO[0000] ✅ Exists: StatefulSet "noobaa-core"
INFO[0000] ❌ Not Found: StatefulSet "noobaa-db"
INFO[0000] ✅ Exists: Service "noobaa-mgmt"
INFO[0000] ✅ Exists: Service "s3"
INFO[0000] ❌ Not Found: Service "noobaa-db"
INFO[0000] ✅ Exists: Secret "noobaa-server"
INFO[0000] ✅ Exists: Secret "noobaa-operator"
INFO[0000] ❌ Not Found: Secret "noobaa-endpoints"
INFO[0000] ✅ Exists: Secret "noobaa-admin"
INFO[0000] ✅ Exists: StorageClass "openshift-storage.noobaa.io"
INFO[0000] ✅ Exists: BucketClass "noobaa-default-bucket-class"
INFO[0000] ❌ Not Found: Deployment "noobaa-endpoint"
INFO[0000] ❌ Not Found: HorizontalPodAutoscaler "noobaa-endpoint"
INFO[0000] ✅ (Optional) Exists: BackingStore "noobaa-default-backing-store"
INFO[0000] ✅ (Optional) Exists: CredentialsRequest "noobaa-cloud-creds"
INFO[0000] ✅ (Optional) Exists: PrometheusRule "noobaa-prometheus-rules"
INFO[0000] ✅ (Optional) Exists: ServiceMonitor "noobaa-service-monitor"
INFO[0000] ✅ (Optional) Exists: Route "noobaa-mgmt"
INFO[0000] ✅ (Optional) Exists: Route "s3"
INFO[0000] ❌ Not Found: PersistentVolumeClaim "db-noobaa-db-0"
INFO[0000] ✅ System Phase is "Ready"
INFO[0000] ✅ Exists:  "noobaa-admin"

#------------------#
#- Mgmt Addresses -#
#------------------#

ExternalDNS : [https://aa554dd48798d11ea82360ac586f72e0-1419344470.us-east-1.elb.amazonaws.com:443]
ExternalIP  : []
NodePorts   : [https://10.0.135.241:32402]
InternalDNS : [https://noobaa-mgmt.openshift-storage.svc:443]
InternalIP  : [https://172.30.198.57:443]
PodPorts    : [https://10.129.2.12:8443]

#--------------------#
#- Mgmt Credentials -#
#--------------------#

email    : <NOT REDACTED>
password : <NOT REDACTED>

#----------------#
#- S3 Addresses -#
#----------------#

ExternalDNS : [https://aa559ab36798d11ea82360ac586f72e0-1677939048.us-east-1.elb.amazonaws.com:443]
ExternalIP  : []
NodePorts   : [https://10.0.135.241:30936]
InternalDNS : [https://s3.openshift-storage.svc:443]
InternalIP  : [https://172.30.70.148:443]
PodPorts    : [https://10.129.2.12:6443]

#------------------#
#- S3 Credentials -#
#------------------#

AWS_ACCESS_KEY_ID     : <NOT REDACTED>
AWS_SECRET_ACCESS_KEY : <NOT REDACTED>

#------------------#
#- Backing Stores -#
#------------------#

NAME                           TYPE     TARGET-BUCKET                                               PHASE   AGE
noobaa-default-backing-store   aws-s3   noobaa-backing-store-6411fc43-6d97-4b98-ab4e-f95caa5a24c5   Ready   54h43m1s

#------------------#
#- Bucket Classes -#
#------------------#

NAME                          PLACEMENT                                                             PHASE   AGE
noobaa-default-bucket-class   {Tiers:[{Placement: BackingStores:[noobaa-default-backing-store]}]}   Ready   54h43m1s

#-----------------#
#- Bucket Claims -#
#-----------------#

No OBCs found.
{% endhighlight %}


Now lets create an OBC called test21obc in the openshift-storage namespace.

{% highlight console %}
[maur0x@clientvm 22 ~]$ noobaa obc create test21obc -n openshift-storage
INFO[0000] ✅ Exists: StorageClass "openshift-storage.noobaa.io"
INFO[0000] ✅ Created: ObjectBucketClaim "test21obc"
INFO[0000]
INFO[0000] NOTE:
INFO[0000]   - This command has finished applying changes to the cluster.
INFO[0000]   - From now on, it only loops and reads the status, to monitor the operator work.
INFO[0000]   - You may Ctrl-C at any time to stop the loop and watch it manually.
INFO[0000]
INFO[0000] OBC Wait Ready:
INFO[0000] ⏳ OBC "test21obc" Phase is ""
INFO[0003] ✅ OBC "test21obc" Phase is Bound
INFO[0003]
INFO[0003]
INFO[0003] ✅ Exists: ObjectBucketClaim "test21obc"
INFO[0003] ✅ Exists: ObjectBucket "obc-openshift-storage-test21obc"
INFO[0003] ✅ Exists: ConfigMap "test21obc"
INFO[0003] ✅ Exists: Secret "test21obc"
INFO[0003] ✅ Exists: StorageClass "openshift-storage.noobaa.io"
INFO[0003] ✅ Exists: BucketClass "noobaa-default-bucket-class"
INFO[0003] ✅ Exists: NooBaa "noobaa"
INFO[0003] ✅ Exists: Service "noobaa-mgmt"
INFO[0003] ✅ Exists: Secret "noobaa-operator"
INFO[0003] ✅ Exists: Secret "noobaa-admin"
INFO[0003] ✈️  RPC: bucket.read_bucket() Request: {Name:test21obc-a67f47c1-d964-424a-9eaa-cf628b198368}
WARN[0003] RPC: GetConnection creating connection to wss://localhost:45060/rpc/ 0xc0006a8b40
INFO[0003] RPC: Connecting websocket (0xc0006a8b40) &{RPC:0xc0000ba8c0 Address:wss://localhost:45060/rpc/ State:init WS:<nil> PendingRequests:map[] NextRequestID:0 Lock:{state:1 sema:0} ReconnectDelay:0s}
INFO[0003] RPC: Connected websocket (0xc0006a8b40) &{RPC:0xc0000ba8c0 Address:wss://localhost:45060/rpc/ State:init WS:<nil> PendingRequests:map[] NextRequestID:0 Lock:{state:1 sema:0} ReconnectDelay:0s}
INFO[0003] ✅ RPC: bucket.read_bucket() Response OK: took 10.6ms

ObjectBucketClaim info:
  Phase                  : Bound
  ObjectBucketClaim      : kubectl get -n openshift-storage objectbucketclaim test21obc
  ConfigMap              : kubectl get -n openshift-storage configmap test21obc
  Secret                 : kubectl get -n openshift-storage secret test21obc
  ObjectBucket           : kubectl get objectbucket obc-openshift-storage-test21obc
  StorageClass           : kubectl get storageclass openshift-storage.noobaa.io
  BucketClass            : kubectl get -n openshift-storage bucketclass noobaa-default-bucket-class

Connection info:
  BUCKET_HOST            : 10.0.135.241
  BUCKET_NAME            : test21obc-a67f47c1-d964-424a-9eaa-cf628b198368
  BUCKET_PORT            : 30936
  AWS_ACCESS_KEY_ID      : <NOT REDACTED>
  AWS_SECRET_ACCESS_KEY  : <NOT_REDACTED>

Shell commands:
  AWS S3 Alias           : alias s3='AWS_ACCESS_KEY_ID=<NOT REDACTED> AWS_SECRET_ACCESS_KEY=<NOT REDACTED> aws s3 --no-verify-ssl --endpoint-url https://10.0.135.241:30936'

Bucket status:
  Name                   : test21obc-a67f47c1-d964-424a-9eaa-cf628b198368
  Type                   : REGULAR
  Mode                   : OPTIMAL
  ResiliencyStatus       : OPTIMAL
  QuotaStatus            : QUOTA_NOT_SET
  Num Objects            : 0
  Data Size              : 0.000 B
  Data Size Reduced      : 0.000 B
  Data Space Avail       : 1.000 PB
{% endhighlight %}

Check the new OBC.

{% highlight console %}
[maur0x@clientvm 0 ~]$ oc get obc -n openshift-storage
NAME        STORAGE-CLASS                 PHASE   AGE
test21obc   openshift-storage.noobaa.io   Bound   22s

[maur0x@clientvm 0 ~]$ oc get obc test21obc -o yaml -n openshift-storage
apiVersion: objectbucket.io/v1alpha1
kind: ObjectBucketClaim
metadata:
  creationTimestamp: "2020-04-10T19:13:58Z"
  finalizers:
  - objectbucket.io/finalizer
  generation: 2
  labels:
    app: noobaa
    bucket-provisioner: openshift-storage.noobaa.io-obc
    noobaa-domain: openshift-storage.noobaa.io
  name: test21obc
  namespace: openshift-storage
  resourceVersion: "1124360"
  selfLink: /apis/objectbucket.io/v1alpha1/namespaces/openshift-storage/objectbucketclaims/test21obc
  uid: 6dfcd971-7b5f-11ea-be07-0ac586f72e0b
spec:
  ObjectBucketName: obc-openshift-storage-test21obc
  bucketName: test21obc-a67f47c1-d964-424a-9eaa-cf628b198368
  generateBucketName: test21obc
  storageClassName: openshift-storage.noobaa.io
status:
  phase: Bound
{% endhighlight %}

Check the OBC secret holding the access credentials.

{% highlight console %}
[maur0x@clientvm 0 ~]$ oc get -n openshift-storage secret test21obc -o yaml
apiVersion: v1
data:
  AWS_ACCESS_KEY_ID: <NOT REDACTED>
  AWS_SECRET_ACCESS_KEY: <NOT REDACTED>
kind: Secret
metadata:
  creationTimestamp: "2020-04-10T19:13:58Z"
  finalizers:
  - objectbucket.io/finalizer
  labels:
    app: noobaa
    bucket-provisioner: openshift-storage.noobaa.io-obc
    noobaa-domain: openshift-storage.noobaa.io
  name: test21obc
  namespace: openshift-storage
  ownerReferences:
  - apiVersion: objectbucket.io/v1alpha1
    blockOwnerDeletion: true
    controller: true
    kind: ObjectBucketClaim
    name: test21obc
    uid: 6dfcd971-7b5f-11ea-be07-0ac586f72e0b
  resourceVersion: "1124355"
  selfLink: /api/v1/namespaces/openshift-storage/secrets/test21obc
  uid: 6e1372aa-7b5f-11ea-abc4-1277afaa9b89
type: Opaque
{% endhighlight %}

And the config map with the bucket details.

{% highlight console %}
[maur0x@clientvm 0 ~]$ oc get -n openshift-storage cm test21obc -o yaml
apiVersion: v1
data:
  BUCKET_HOST: 10.0.135.241
  BUCKET_NAME: test21obc-a67f47c1-d964-424a-9eaa-cf628b198368
  BUCKET_PORT: "30936"
  BUCKET_REGION: ""
  BUCKET_SUBREGION: ""
kind: ConfigMap
metadata:
  creationTimestamp: "2020-04-10T19:13:58Z"
  finalizers:
  - objectbucket.io/finalizer
  labels:
    app: noobaa
    bucket-provisioner: openshift-storage.noobaa.io-obc
    noobaa-domain: openshift-storage.noobaa.io
  name: test21obc
  namespace: openshift-storage
  ownerReferences:
  - apiVersion: objectbucket.io/v1alpha1
    blockOwnerDeletion: true
    controller: true
    kind: ObjectBucketClaim
    name: test21obc
    uid: 6dfcd971-7b5f-11ea-be07-0ac586f72e0b
  resourceVersion: "1124356"
  selfLink: /api/v1/namespaces/openshift-storage/configmaps/test21obc
  uid: 6e1b97a0-7b5f-11ea-abc4-1277afaa9b89
{% endhighlight %}

With this information now we can create a sample application to use test21obc, which is the aws-cli container image that holds the **s3cmd** S3 client, and runs a **du** sub-command against our bucket.

{% highlight console %}
[maur0x@clientvm 0 ~]$ curl -s https://raw.githubusercontent.com/red-hat-storage/ocs-training/master/ocp4ocs4/obc_app_example.yaml | oc apply -f -
namespace/obc-test created
objectbucketclaim.objectbucket.io/obc-test created
job.batch/obc-test created
{% endhighlight %}


{% highlight console %}
[maur0x@clientvm 0 ~]$ oc get pods -n obc-test -l app=obc-test
NAME             READY   STATUS      RESTARTS   AGE
obc-test-4sjss   0/1     Completed   0          33s

[maur0x@clientvm 0 ~]$ kubectl logs -n obc-test -l app=obc-test
+ s3cmd --no-check-certificate --host 10.0.135.241:30936 --host-bucket 10.0.135.241:30936 du
0        0 objects s3://obc-test-noobaa-e4b7e9f6-2e7e-4ca3-8a2a-d6729be03c17/
--------
0        Total
{% endhighlight %}


### Scale Out OCS

In this section we can scale out the machinesets for OCS to 2 replicas each. Thus 6 OSD nodes and OSD disks.

Check current state.
{% highlight console %}
[maur0x@clientvm 0 ~]$ oc get machinesets -n openshift-machine-api | egrep 'NAME|workerocs'
NAME                                          DESIRED   CURRENT   READY   AVAILABLE   AGE
cluster-ocs-e388-c84n5-workerocs-us-east-1a   1         1         1       1           2d11h
cluster-ocs-e388-c84n5-workerocs-us-east-1b   1         1         1       1           2d11h
cluster-ocs-e388-c84n5-workerocs-us-east-1c   1         1         1       1           2d11h
{% endhighlight %}

Scale out the machinesets.
{% highlight console %}
[maur0x@clientvm 0 ~]$ oc get machinesets -n openshift-machine-api -o name | grep workerocs | xargs -n1 -t oc scale -n openshift-machine-api --replicas=2
oc scale -n openshift-machine-api --replicas=2 machineset.machine.openshift.io/cluster-ocs-e388-c84n5-workerocs-us-east-1a
machineset.machine.openshift.io/cluster-ocs-e388-c84n5-workerocs-us-east-1a scaled
oc scale -n openshift-machine-api --replicas=2 machineset.machine.openshift.io/cluster-ocs-e388-c84n5-workerocs-us-east-1b
machineset.machine.openshift.io/cluster-ocs-e388-c84n5-workerocs-us-east-1b scaled
oc scale -n openshift-machine-api --replicas=2 machineset.machine.openshift.io/cluster-ocs-e388-c84n5-workerocs-us-east-1c
machineset.machine.openshift.io/cluster-ocs-e388-c84n5-workerocs-us-east-1c scaled
{% endhighlight %}

After a while we have 6 OCS worker nodes. Two in each AZ.

{% highlight console %}
[maur0x@clientvm 0 ~]$ oc get machines -n openshift-machine-api
NAME                                                STATE     TYPE         REGION      ZONE         AGE
cluster-ocs-e388-c84n5-master-0                     running   m5.xlarge    us-east-1   us-east-1a   2d13h
cluster-ocs-e388-c84n5-master-1                     running   m5.xlarge    us-east-1   us-east-1b   2d13h
cluster-ocs-e388-c84n5-master-2                     running   m5.xlarge    us-east-1   us-east-1c   2d13h
cluster-ocs-e388-c84n5-worker-us-east-1a-7mndb      running   m5.4xlarge   us-east-1   us-east-1a   2d13h
cluster-ocs-e388-c84n5-worker-us-east-1b-bd5md      running   m5.4xlarge   us-east-1   us-east-1b   2d13h
cluster-ocs-e388-c84n5-workerocs-us-east-1a-97px4   running   m4.4xlarge   us-east-1   us-east-1a   2d11h
cluster-ocs-e388-c84n5-workerocs-us-east-1a-l2jkc   running   m4.4xlarge   us-east-1   us-east-1a   53s
cluster-ocs-e388-c84n5-workerocs-us-east-1b-92xt7   running   m4.4xlarge   us-east-1   us-east-1b   2d11h
cluster-ocs-e388-c84n5-workerocs-us-east-1b-bfjkv   running   m4.4xlarge   us-east-1   us-east-1b   53s
cluster-ocs-e388-c84n5-workerocs-us-east-1c-7n4s4   running   m4.4xlarge   us-east-1   us-east-1c   2d11h
cluster-ocs-e388-c84n5-workerocs-us-east-1c-bgtnz   running   m4.4xlarge   us-east-1   us-east-1c   52s

[maur0x@clientvm 1 ~]$ oc get machines -n openshift-machine-api -o name | grep workerocs
machine.machine.openshift.io/cluster-ocs-e388-c84n5-workerocs-us-east-1a-97px4
machine.machine.openshift.io/cluster-ocs-e388-c84n5-workerocs-us-east-1a-l2jkc
machine.machine.openshift.io/cluster-ocs-e388-c84n5-workerocs-us-east-1b-92xt7
machine.machine.openshift.io/cluster-ocs-e388-c84n5-workerocs-us-east-1b-bfjkv
machine.machine.openshift.io/cluster-ocs-e388-c84n5-workerocs-us-east-1c-7n4s4
machine.machine.openshift.io/cluster-ocs-e388-c84n5-workerocs-us-east-1c-bgtnz

[maur0x@clientvm 130 ~]$ oc get nodes
NAME                           STATUS   ROLES    AGE     VERSION
ip-10-0-129-31.ec2.internal    Ready    worker   2d13h   v1.14.6+70aebec30
ip-10-0-135-241.ec2.internal   Ready    worker   2d11h   v1.14.6+70aebec30
ip-10-0-139-100.ec2.internal   Ready    master   2d13h   v1.14.6+70aebec30
ip-10-0-146-22.ec2.internal    Ready    worker   2d11h   v1.14.6+70aebec30
ip-10-0-158-199.ec2.internal   Ready    master   2d13h   v1.14.6+70aebec30
ip-10-0-159-132.ec2.internal   Ready    worker   2d13h   v1.14.6+70aebec30
ip-10-0-160-178.ec2.internal   Ready    master   2d13h   v1.14.6+70aebec30
ip-10-0-162-88.ec2.internal    Ready    worker   2d11h   v1.14.6+70aebec30

[maur0x@clientvm 0 ~]$ oc get pod -o=custom-columns=NAME:.metadata.name,STATUS:.status.phase,NODE:.spec.nodeName -n openshift-storage | grep osd
rook-ceph-osd-0-77c5cccc9-8b65x                                   Running     ip-10-0-162-88.ec2.internal
rook-ceph-osd-1-78b9f54d65-hnb6v                                  Running     ip-10-0-135-241.ec2.internal
rook-ceph-osd-2-5674b7768f-gzm6h                                  Running     ip-10-0-146-22.ec2.internal
rook-ceph-osd-prepare-ocs-deviceset-0-0-42759-9vl64               Succeeded   ip-10-0-135-241.ec2.internal
rook-ceph-osd-prepare-ocs-deviceset-0-1-8v8z6-8xsgl               Pending     ip-10-0-135-241.ec2.internal
rook-ceph-osd-prepare-ocs-deviceset-1-0-t4ks6-4gdr5               Succeeded   ip-10-0-162-88.ec2.internal
rook-ceph-osd-prepare-ocs-deviceset-1-1-qtvmc-9ps5f               Pending     ip-10-0-162-88.ec2.internal
rook-ceph-osd-prepare-ocs-deviceset-2-0-cbwf6-k9c5s               Succeeded   ip-10-0-146-22.ec2.internal
rook-ceph-osd-prepare-ocs-deviceset-2-1-npmtw-z794w               Pending     ip-10-0-146-22.ec2.internal

[maur0x@clientvm 0 ~]$ oc get pod -o=custom-columns=NAME:.metadata.name,STATUS:.status.phase,NODE:.spec.nodeName -n openshift-storage | grep osd
rook-ceph-osd-0-77c5cccc9-8b65x                                   Running     ip-10-0-162-88.ec2.internal
rook-ceph-osd-1-78b9f54d65-hnb6v                                  Running     ip-10-0-135-241.ec2.internal
rook-ceph-osd-2-5674b7768f-gzm6h                                  Running     ip-10-0-146-22.ec2.internal
rook-ceph-osd-3-7495c84544-hbw4m                                  Running     ip-10-0-135-241.ec2.internal
rook-ceph-osd-4-559f88d48-h7r5x                                   Running     ip-10-0-162-88.ec2.internal
rook-ceph-osd-5-6c7bb86597-pxz2d                                  Running     ip-10-0-146-22.ec2.internal
rook-ceph-osd-prepare-ocs-deviceset-0-0-42759-9vl64               Succeeded   ip-10-0-135-241.ec2.internal
rook-ceph-osd-prepare-ocs-deviceset-0-1-8v8z6-8xsgl               Succeeded   ip-10-0-135-241.ec2.internal
rook-ceph-osd-prepare-ocs-deviceset-1-0-t4ks6-4gdr5               Succeeded   ip-10-0-162-88.ec2.internal
rook-ceph-osd-prepare-ocs-deviceset-1-1-qtvmc-9ps5f               Succeeded   ip-10-0-162-88.ec2.internal
rook-ceph-osd-prepare-ocs-deviceset-2-0-cbwf6-k9c5s               Succeeded   ip-10-0-146-22.ec2.internal
rook-ceph-osd-prepare-ocs-deviceset-2-1-npmtw-z794w               Succeeded   ip-10-0-146-22.ec2.internal
{% endhighlight %}

Now that the OSDs are ready lets see them reported in the Ceph cluster.

{% highlight console %}
[maur0x@clientvm 0 ~]$ oc rsh rook-ceph-operator-bcc7f79ff-v4cxj
sh-4.4$ ceph -s
  cluster:
    id:     736b621b-c423-49c7-a664-44f54e2134cd
    health: HEALTH_OK

  services:
    mon: 3 daemons, quorum a,b,c (age 2d)
    mgr: a(active, since 2d)
    mds: ocs-storagecluster-cephfilesystem:1 {0=ocs-storagecluster-cephfilesystem-b=up:active} 1 up:standby-replay
    osd: 6 osds: 6 up (since 32s), 6 in (since 32s)

  data:
    pools:   3 pools, 24 pgs
    objects: 842 objects, 2.4 GiB
    usage:   12 GiB used, 12 TiB / 12 TiB avail
    pgs:     24 active+clean

  io:
    client:   853 B/s rd, 71 KiB/s wr, 1 op/s rd, 2 op/s wr


sh-4.4$ ceph osd crush tree
ID  CLASS WEIGHT   TYPE NAME
 -1       11.99396 root default
 -5       11.99396     region us-east-1
-10        3.99799         zone us-east-1a
 -9        1.99899             host ocs-deviceset-0-0-42759
  1   ssd  1.99899                 osd.1
-17        1.99899             host ocs-deviceset-0-1-8v8z6
  3   ssd  1.99899                 osd.3
-14        3.99799         zone us-east-1b
-13        1.99899             host ocs-deviceset-2-0-cbwf6
  2   ssd  1.99899                 osd.2
-21        1.99899             host ocs-deviceset-2-1-npmtw
  5   ssd  1.99899                 osd.5
 -4        3.99799         zone us-east-1c
 -3        1.99899             host ocs-deviceset-1-0-t4ks6
  0   ssd  1.99899                 osd.0
-19        1.99899             host ocs-deviceset-1-1-qtvmc
  4   ssd  1.99899                 osd.4
sh-4.4$ exit
{% endhighlight %}


### References

[^1]: <https://rook.io>
[^2]: <https://noobaa.io>

